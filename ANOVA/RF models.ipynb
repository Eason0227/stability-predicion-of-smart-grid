{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"XaDLqVcOJZwZ"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import warnings\n","import math\n","warnings.filterwarnings('ignore')\n","\n","from sklearn.tree import DecisionTreeRegressor\n","from sklearn.model_selection import train_test_split\n","from sklearn.model_selection import KFold\n","from sklearn.preprocessing import MinMaxScaler,StandardScaler\n","from sklearn.metrics import mean_squared_error\n","from sklearn.ensemble import RandomForestRegressor\n","from scipy.stats import shapiro,f_oneway,kruskal\n","\n","data = pd.read_csv(\"Electrical Grid.csv\")\n","\n","data_x = data.drop(['stab','stabf'],axis=1)\n","data_y = data['stab']\n","\n","scaler = MinMaxScaler()\n","scaled_x = scaler.fit_transform(data_x)\n","scaled_x = pd.DataFrame(scaled_x, columns=data_x.columns)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tJ_yjZ5xKCG2"},"outputs":[],"source":["depth_data = pd.read_csv(\"depth.csv\")\n","tree_data = pd.read_csv(\"tree.csv\")\n","max_feature_data = pd.read_csv(\"max feature.csv\")\n","bootstrap_size_data = pd.read_csv(\"bootstrap size.csv\")\n","batch_data = pd.read_csv(\"batch size.csv\")\n","iteration_data = pd.read_csv(\"iteration.csv\")\n","\n","best_parameter = pd.read_csv(\"bast parameter.csv\")\n","anova_data = pd.read_csv(\"anova result.csv\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lXmMT6YsJ9Sy"},"outputs":[],"source":["def normal_test(data):\n","    test_stat,p_value = shapiro(data)\n","    return np.round(p_value,3)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AzDJ1rtbKBj0"},"outputs":[],"source":["rf_paremater_set = {\"trees\":[100,300, 500, 700],\"sampling_sizes\":[0.5,0.7,0.9],'depths':[10,20,30,40,50],'features':[4,6,8,10,12]}\n","rotf_paremater_set = {\"trees\":[100,300,500,700],\"sampling_sizes\":[0.5,0.7,0.9],'depths':[10,20,30,40,50]}\n","spca_rotf_paremater_set = {\"trees\": [300, 500, 700, 900], \"sampling_sizes\": [0.5, 0.7, 0.9], 'depths': [10, 20, 30, 40, 50],\n","               'batch': [3, 5, 7, 9], 'iterations ': [100, 300, 500, 700]}"]},{"cell_type":"markdown","metadata":{"id":"rmUbRhcYlQ1B"},"source":["# RF"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zCD4wA-kNJiy"},"outputs":[],"source":["def rf_kfold(tree,depth,max_sample,feature,seed):\n","    RMSE_set_rf = []\n","    kf = KFold(n_splits=10, shuffle=True, random_state=seed)\n","    for train_index, valid_index in kf.split(scaled_x, data_y): # split()  method generate indices to split data intSo training and test set.\n","        #print('fold',cnt)\n","        train_x = scaled_x.iloc[train_index,:]\n","        train_y = data.iloc[train_index,:]['stab']\n","        valid_x = scaled_x.iloc[valid_index,:]\n","        valid_y = data.iloc[valid_index,:]['stab']\n","        rf_pred = RandomForestRegressor(n_estimators=tree,max_depth=depth,max_samples=max_sample,n_jobs=-1,max_features=feature).fit(train_x,train_y).predict(valid_x)\n","        RMSE_set_rf.append( np.sqrt( mean_squared_error(valid_y,rf_pred)))\n","    return RMSE_set_rf"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":593},"id":"UfjbH1gcNZ1B","outputId":"72835348-ec08-4922-a150-76fca1b2710d"},"outputs":[{"name":"stdout","output_type":"stream","text":["================ trees ================\n","100 0.365 [0.011957633267011464, 0.011816719016102302, 0.011816374254953807, 0.0115751927958254, 0.011883251048778163, 0.011704967605190951, 0.011897351007726195, 0.011810037824681438, 0.011553465513726012, 0.011769632382042452]\n","300 0.046 [0.011821636198734551, 0.011835353276957127, 0.01174495338871394, 0.011491388017077064, 0.01177933161232889, 0.011557129350539685, 0.011797840903428131, 0.011615135362457408, 0.011502829765341645, 0.011788256916596882]\n","500 0.061 [0.01174678215489854, 0.011780438733438313, 0.011719019652598157, 0.011517342519258867, 0.011732047460743652, 0.011477023387818691, 0.011836157901903701, 0.011536893152410785, 0.01149468618712398, 0.011792094729034454]\n","700 0.107 [0.011755563967699544, 0.0117148009325921, 0.011704120879376955, 0.011419787389937841, 0.011720892618359307, 0.011532435915159026, 0.011817635430019756, 0.011480624313067083, 0.011474235460417181, 0.011774654693396721]\n","kruskal_oneway: 6.604390243902429 0.08563528359403802\n","================ sampling_sizes ================\n","0.5 0.924 [0.01290833000116219, 0.012747726882286974, 0.012800573980504673, 0.012542377891887363, 0.012620260449799341, 0.012356242324852829, 0.01299098364657563, 0.012768977302038836, 0.012623684112227425, 0.012624264171644427]\n","0.7 0.59 [0.012483435023087028, 0.012513707473242751, 0.012450217238400138, 0.01211151762953359, 0.012172500318446787, 0.012018389504559841, 0.012635137642389632, 0.012390643230927227, 0.012209521833902478, 0.012182467274357336]\n","0.9 0.671 [0.012285599425614371, 0.012309931303180657, 0.012230606308541876, 0.01185116691402979, 0.012056149326331717, 0.011845160183632086, 0.012437022400467882, 0.012211547097680306, 0.012147737464025036, 0.012025044226588339]\n","f_oneway: 21.532489126635465 2.5653547515185844e-06\n","================ depths ================\n","10 0.521 [0.013003797799501275, 0.01287778570724243, 0.012829526908316983, 0.012331906062566449, 0.012287415553375411, 0.012601604038575681, 0.012065136088527979, 0.01245162971319524, 0.01238437441499327, 0.013562234126510114]\n","20 0.447 [0.012149311318330887, 0.011918212550595957, 0.011923371204338942, 0.011462502318004246, 0.01139796189605524, 0.011590390725963734, 0.011147958814197606, 0.011585759159957881, 0.01144409430232192, 0.01261525328430528]\n","30 0.684 [0.012126462610515877, 0.011896545621143199, 0.011895485142126117, 0.011402087189830283, 0.011311796275629506, 0.011703014773794327, 0.011136175586691043, 0.011542736320486904, 0.011424408204997382, 0.012558345335599335]\n","40 0.314 [0.01203741558617516, 0.011931728901966221, 0.01194418972283012, 0.011414017429775714, 0.011348031893448818, 0.011715583330631938, 0.01122315024587056, 0.011464383881842902, 0.01147484736105968, 0.012573910050140323]\n","50 0.339 [0.012141581501013037, 0.011851139474235602, 0.011972011043633686, 0.011427420565354838, 0.011386033405679927, 0.011584643346941564, 0.011222525343491668, 0.011540520598493307, 0.011492535478872159, 0.012569605697690734]\n","F_onewayResult(statistic=9.515856536812636, pvalue=1.1588913049274821e-05)\n","================ features ================\n","4 0.737 [0.012760546232506075, 0.012689970910069993, 0.012697621345750705, 0.012077294062467063, 0.011995348106335476, 0.012210012968323256, 0.011734297413030828, 0.012399028315854895, 0.012072727526879537, 0.013192136215399514]\n","6 0.851 [0.012346467448561584, 0.012222797471363811, 0.012188073650727039, 0.011637655566294038, 0.011483586360392413, 0.011859425316393148, 0.011284930891998056, 0.011907859241552547, 0.011513648202678382, 0.012670353679089249]\n","8 0.533 [0.012100699772971204, 0.01199480121210558, 0.012122123776965109, 0.011519882130518522, 0.011405337943765385, 0.011734431019660476, 0.011232548971998768, 0.011636890529058386, 0.011381304481505433, 0.012660402603627108]\n","10 0.377 [0.012050310188485739, 0.011923719844375255, 0.01193388919473287, 0.011469078896172826, 0.011355644008373031, 0.011641110336676141, 0.011182806262568831, 0.011518207019118112, 0.011429735432104534, 0.012611024242564219]\n","12 0.556 [0.012072297998248294, 0.011958813958443913, 0.011879621390593568, 0.011482917383059657, 0.011309834028005275, 0.011632159912495811, 0.011195722438568095, 0.011558848644913466, 0.011410949600860365, 0.012557499154956324]\n","F_onewayResult(statistic=4.300763229766854, pvalue=0.0049569633395698635)\n","[0.01177846247160382, 0.01169338547921753, 0.011663248587922915, 0.011639475160002553]\n","[0.012698342076297966, 0.01231675371688468, 0.012139996465009207]\n","[0.012639541041280483, 0.01172348155740717, 0.011699705706081396, 0.011712725840374142, 0.011718801645540653]\n","[0.012382898309661734, 0.011911479782905026, 0.011778842244217597, 0.011711552542517155, 0.011705866451014476]\n"]}],"source":["for param_name, parameter in rf_paremater_set.items():\n","    print('================',param_name,'================')\n","    if param_name == 'trees':\n","        tree_rmse  = []\n","        best_rmse,best_para = 100,1000\n","        min_normality_p = 100\n","        for para in parameter:\n","            parm_set_2 = rf_kfold(tree=para,depth=None,max_sample=0.7,feature=12,seed = 47 ) #23 31 47 52\n","            print(para,normal_test(parm_set_2),parm_set_2)\n","\n","            # 找出最好的樹參數\n","            if np.mean(parm_set_2) < best_rmse:\n","              best_tree = para\n","              best_rmse = np.mean(parm_set_2)\n","            tree_rmse.append(parm_set_2)\n","\n","            # 紀錄每個參數的常態檢定p-value\n","            tree_data[str(para)].iloc[0] = normal_test(parm_set_2)\n","\n","            if normal_test(parm_set_2) < min_normality_p:\n","              min_normality_p = normal_test(parm_set_2)\n","\n","        if min_normality_p < 0.05 :\n","          test_stat,anova_p = kruskal(tree_rmse[0],tree_rmse[1],tree_rmse[2],tree_rmse[3])\n","          print('kruskal_oneway:',test_stat,anova_p)\n","        else:\n","          test_stat,anova_p = f_oneway(tree_rmse[0],tree_rmse[1],tree_rmse[2],tree_rmse[3])\n","          print('f_oneway:',test_stat,anova_p)\n","\n","        anova_data['trees'].iloc[0] = np.round(anova_p,3)    # 紀錄該參數的anova result\n","        best_parameter['trees'].iloc[0] = best_tree   # 紀錄該參數的最佳參數\n","        tree_result = [np.mean(tree_rmse[0]),np.mean(tree_rmse[1]),np.mean(tree_rmse[2]),np.mean(tree_rmse[3])]   # 紀錄每個參數之平均rmse\n","\n","    elif param_name == 'sampling_sizes':\n","        sample_rmse = []\n","        best_rmse,best_para = 100,1000\n","        min_normality_p = 100\n","        for para in parameter:\n","            parm_set_2 = rf_kfold(tree=300,depth=20,max_sample=para,feature=4,seed = 47)\n","            print(para,normal_test(parm_set_2),parm_set_2)\n","\n","            # 找出最好的參數\n","            if np.mean(parm_set_2) < best_rmse:\n","              best_para = para\n","              best_rmse = np.mean(parm_set_2)\n","            sample_rmse.append(parm_set_2)\n","\n","            # 紀錄每個參數的常態檢定p-value\n","            bootstrap_size_data[str(para)].iloc[0] = normal_test(parm_set_2)\n","\n","            if normal_test(parm_set_2) < min_normality_p:\n","              min_normality_p = normal_test(parm_set_2)\n","\n","        if min_normality_p < 0.05 :\n","          test_stat,anova_p = kruskal(sample_rmse[0],sample_rmse[1],sample_rmse[2])\n","          print('kruskal_oneway:',test_stat,anova_p)\n","        else:\n","          test_stat,anova_p = f_oneway(sample_rmse[0],sample_rmse[1],sample_rmse[2])\n","          print('f_oneway:',test_stat,anova_p)\n","\n","        anova_data['bootstrap size'].iloc[0] = np.round(anova_p,3)\n","\n","        best_parameter['bootstrap size'].iloc[0] = best_para   # 紀錄該參數的最佳參數\n","        bootstrap_result = [np.mean(sample_rmse[0]),np.mean(sample_rmse[1]),np.mean(sample_rmse[2])]  # 紀錄每個參數之平均rmse\n","\n","    elif param_name == 'depths':\n","        depth_rmse = []\n","        best_rmse,best_para = 100,1000\n","        for para in parameter:\n","            parm_set_2 = rf_kfold(tree=300,depth=para,max_sample=0.7,feature='auto',seed = 42)\n","            print(para,normal_test(parm_set_2),parm_set_2)\n","\n","            # 找出最好的參數\n","            if np.mean(parm_set_2) < best_rmse:\n","              best_para = para\n","              best_rmse = np.mean(parm_set_2)\n","            depth_rmse.append(parm_set_2)\n","\n","            # 紀錄每個參數的常態檢定p-value\n","            depth_data[str(para)].iloc[0] = normal_test(parm_set_2)\n","\n","        print(f_oneway(depth_rmse[0],depth_rmse[1],depth_rmse[2],depth_rmse[3],depth_rmse[4]))\n","\n","        # 紀錄該參數的anova result\n","        test_stat,anova_p = f_oneway(depth_rmse[0],depth_rmse[1],depth_rmse[2],depth_rmse[3],depth_rmse[4])\n","        anova_data['max depth'].iloc[0] = np.round(anova_p,3)\n","\n","        # 紀錄該參數的最佳參數\n","        best_parameter['max depth'].iloc[0] = best_para\n","\n","        # 紀錄每個參數之平均rmse\n","        depth_result = [np.mean(depth_rmse[0]),np.mean(depth_rmse[1]),np.mean(depth_rmse[2]),np.mean(depth_rmse[3]),np.mean(depth_rmse[4])]\n","\n","    elif param_name == 'features':\n","        feature_rmse= []\n","        best_rmse,best_para = 100,1000\n","        for para in parameter:\n","            parm_set_2 = rf_kfold(tree=300,depth=30,max_sample=0.7,feature=para,seed = 42)\n","            print(para,normal_test(parm_set_2),parm_set_2)\n","\n","            # 找出最好的參數\n","            if np.mean(parm_set_2) < best_rmse:\n","              best_para = para\n","              best_rmse = np.mean(parm_set_2)\n","            feature_rmse.append(parm_set_2)\n","\n","            # 紀錄每個參數的常態檢定p-value\n","            max_feature_data[str(para)].iloc[0] = normal_test(parm_set_2)\n","\n","        print(f_oneway(feature_rmse[0],feature_rmse[1],feature_rmse[2],feature_rmse[3],feature_rmse[4]))\n","\n","        # 紀錄該參數的anova result\n","        test_stat,anova_p = f_oneway(feature_rmse[0],feature_rmse[1],feature_rmse[2],feature_rmse[3],feature_rmse[4])\n","        anova_data['max feature'].iloc[0] = np.round(anova_p,3)\n","\n","        # 紀錄該參數的最佳參數\n","        best_parameter['max feature'].iloc[0] = best_para\n","\n","        # 紀錄每個參數之平均rmse\n","        feature_result = [np.mean(feature_rmse[0]),np.mean(feature_rmse[1]),np.mean(feature_rmse[2]),np.mean(feature_rmse[3]),np.mean(feature_rmse[4])]\n","\n","print(tree_result)\n","print(bootstrap_result)\n","print(depth_result)\n","print(feature_result)"]},{"cell_type":"markdown","metadata":{"id":"dGBL8CDElQ1E"},"source":["# TWRF"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"C_wVhS64lQ1E"},"outputs":[],"source":["def TWRF_kfold(tree,depth,max_sample,feature,seed):\n","    RMSE_set_twrf = []\n","    kf = KFold(n_splits=10, shuffle=True, random_state=seed)\n","    for train_index, valid_index in kf.split(scaled_x, data_y): # split()  method generate indices to split data intSo training and test set.\n","        #print('fold',cnt)\n","        train_x = scaled_x.iloc[train_index,:]\n","        train_y = data.iloc[train_index,:]['stab']\n","        valid_x = scaled_x.iloc[valid_index,:]\n","        valid_y = data.iloc[valid_index,:]['stab']\n","\n","        TWRF = Trees_Weighting_Random_Forest(train_x , train_y, valid_x,max_depth=depth,sample_size=max_sample,n_trees=tree, n_features = feature )\n","        RMSE_set_twrf.append( np.sqrt( mean_squared_error(valid_y,TWRF)))\n","\n","    mean_rmse = np.mean(RMSE_set_twrf)\n","    return RMSE_set_twrf\n","\n","def Trees_Weighting_Random_Forest(training_x , training_y, test_x, max_depth, sample_size, n_trees, n_features):\n","    trees = []\n","    OOB_MSE = []\n","    Prediction_set = []\n","    for i in range(n_trees):\n","        train_x, valid_x ,train_y ,valid_y = train_test_split(training_x, training_y , train_size = sample_size) #隨機選擇n%的樣本\n","\n","        DT = DecisionTreeRegressor(max_depth = max_depth , max_features=n_features )\n","        DT.fit(train_x,train_y)\n","        dt_pred = DT.predict(valid_x)\n","\n","        OOB_MSE.append(  mean_squared_error(valid_y,dt_pred) ) #MSE\n","        oob_mse = OOB_MSE/np.sum(OOB_MSE)\n","\n","        test_pred = DT.predict(test_x)\n","        Prediction_set.append(test_pred)\n","\n","    final_results = []\n","    for i in range(len(Prediction_set[0])): #總共要預測的Y有幾個 #2000個預測值\n","        predict_result = [] #存放每棵樹的預測值 #100棵樹有100個預測值\n","        final_result = 0\n","        for j in range(len(Prediction_set)): #每棵樹的預測值 #100棵樹\n","            predict_result.append( (Prediction_set[j][i]) )\n","\n","        for k in range(len(predict_result)): #加權預測值\n","            final_result =  final_result + predict_result[k] * oob_mse[k]\n","        final_results.append(final_result)\n","    return final_results"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IsvU-BgllQ1F","outputId":"1ed1eb8a-8eae-411e-e3a3-29fce67097e0"},"outputs":[{"name":"stdout","output_type":"stream","text":["================ trees ================\n","100 0.537 [0.012013170124137709, 0.011906800939276478, 0.011746788122548727, 0.011730708750787609, 0.011914772063426485, 0.011780762150451023, 0.011942307252324971, 0.011665893234568575, 0.01154575033586865, 0.01193168348142115]\n","300 0.958 [0.011829681996027498, 0.011787558707512096, 0.011859354534901327, 0.011730287656669666, 0.012031733291698187, 0.011727267783458806, 0.011926009921666056, 0.011573728186959092, 0.011620643280726246, 0.011945642578063872]\n","500 0.607 [0.011828194667142772, 0.011772956930445045, 0.011831656726780267, 0.011603411353642437, 0.011993569470439882, 0.011692283591906123, 0.01189062983702028, 0.011577147466866491, 0.01163855619282826, 0.011960277045973878]\n","700 0.377 [0.011782720732406498, 0.011783746449972966, 0.011771671824958951, 0.011664000221204167, 0.011920651007541746, 0.011726674191771211, 0.01189112237326094, 0.011484857835008296, 0.011506783116800558, 0.01195318625451897]\n","f_oneway: 0.40906555953670615 0.7474298419216996\n","================ sampling_sizes ================\n","0.5 0.083 [0.011891770316059315, 0.011908538299552587, 0.011778543393603028, 0.011514562319494668, 0.011927544210490028, 0.011655325231677376, 0.011889162510079982, 0.011666775187979907, 0.011594156156432887, 0.011888200508386747]\n","0.7 0.473 [0.011930039881803231, 0.0117896206127035, 0.011785783851622012, 0.011737428227326966, 0.011924048205922333, 0.011696981703938592, 0.011899559957739997, 0.011564489156312504, 0.011509458035311825, 0.011980742002480626]\n","0.9 0.073 [0.01235913465835147, 0.012267201076547813, 0.0122630658469636, 0.012476978753897914, 0.012878692111653855, 0.012294537852981553, 0.012263965927970799, 0.01262080745166881, 0.012131150572941767, 0.01290561812410136]\n","f_oneway: 36.96314639778853 1.859312893392817e-08\n","================ depths ================\n","10 0.202 [0.01329657840527923, 0.013078422436228476, 0.013243349438043252, 0.012511951060561952, 0.012704438577535844, 0.013020464050256142, 0.012522478067213618, 0.012847823278423369, 0.012583121515998486, 0.013977445528780835]\n","20 0.109 [0.01216173077519162, 0.011855978181040807, 0.012022014979303658, 0.011494946152442901, 0.011431905680587745, 0.011740449771575331, 0.011359964935330449, 0.011572776604726533, 0.011508243242947861, 0.012754356438602234]\n","30 0.258 [0.01221865257218976, 0.011941534159943847, 0.012001615681735205, 0.011441099763852788, 0.011443992792696022, 0.01177053416214685, 0.011366866672474824, 0.011619470165436059, 0.011558370255924913, 0.012661245017485038]\n","40 0.155 [0.01216042879844466, 0.011915899591987298, 0.011948013201715403, 0.011459447889152753, 0.011505186299611753, 0.011747008409163194, 0.011356380050153242, 0.011606729854624926, 0.011458514202235356, 0.012682911927547348]\n","50 0.236 [0.01225592295584764, 0.011907650990608957, 0.011945663220697195, 0.011498177442535541, 0.011490753441517855, 0.011765125073674406, 0.011296849404056104, 0.011624652025905142, 0.011475478198131528, 0.012727369887380768]\n","F_onewayResult(statistic=15.409030316634109, pvalue=5.196607620536195e-08)\n","================ features ================\n","4 0.927 [0.01235526901111028, 0.012150817834143327, 0.012177486345254342, 0.011645432912223815, 0.011609679774924248, 0.011925794710692854, 0.011322768555269338, 0.011982690309363175, 0.011687698809072031, 0.01277807058308492]\n","6 0.53 [0.01211616985441707, 0.011834399171979329, 0.01189266087238912, 0.011315138883569054, 0.011340488879940177, 0.01163757148146296, 0.011094431326672717, 0.011451543013001133, 0.011261906059637259, 0.012535560669545249]\n","8 0.794 [0.012018199310736548, 0.011810634363199289, 0.011868759778508858, 0.011343397525663679, 0.011248208711513288, 0.011649200283790693, 0.01101338110258474, 0.011447661231890201, 0.011375148408905064, 0.012511024852549636]\n","10 0.554 [0.012071932985228585, 0.011861959406767394, 0.011944953298922529, 0.011513833974343066, 0.01131481147367639, 0.011651924780131835, 0.01112962544254025, 0.01137610746738862, 0.011475288956464314, 0.012631510296979448]\n","12 0.172 [0.01215733330898052, 0.011867513506954504, 0.011996580006680552, 0.011501026933604932, 0.011432966941525806, 0.01178758037645947, 0.011394633325264822, 0.0116149315921456, 0.011494209919412966, 0.012632700209814147]\n","F_onewayResult(statistic=1.0179789273459812, pvalue=0.40829028980734255)\n","[0.011817863645481137, 0.011803190793768284, 0.011778868328304545, 0.01174854140074443]\n","[0.011771457813375654, 0.011781815163516158, 0.012446115237707894]\n","[0.01297860723583212, 0.011790236676174914, 0.011802338124388531, 0.011784052022463593, 0.011798764264035512]\n","[0.011963570884513833, 0.011647987021261406, 0.0116285615569342, 0.011697194808244243, 0.011787947612084333]\n"]}],"source":["num = 6\n","\n","for param_name, parameter in rf_paremater_set.items():\n","    print('================',param_name,'================')\n","    if param_name == 'trees':\n","        tree_rmse  = []\n","        best_rmse,best_para = 100,1000\n","        min_normality_p = 100\n","        for para in parameter:\n","            parm_set_2 = TWRF_kfold(tree=para,depth=None,max_sample=0.7,feature=12,seed = 47 ) #23 31 47 52\n","            print(para,normal_test(parm_set_2),parm_set_2)\n","\n","            # 找出最好的樹參數\n","            if np.mean(parm_set_2) < best_rmse:\n","              best_tree = para\n","              best_rmse = np.mean(parm_set_2)\n","            tree_rmse.append(parm_set_2)\n","\n","            # 紀錄每個參數的常態檢定p-value\n","            tree_data[str(para)].iloc[num] = normal_test(parm_set_2)\n","\n","            if normal_test(parm_set_2) < min_normality_p:\n","              min_normality_p = normal_test(parm_set_2)\n","\n","        if min_normality_p < 0.05 :\n","          test_stat,anova_p = kruskal(tree_rmse[0],tree_rmse[1],tree_rmse[2],tree_rmse[3])\n","          print('kruskal_oneway:',test_stat,anova_p)\n","        else:\n","          test_stat,anova_p = f_oneway(tree_rmse[0],tree_rmse[1],tree_rmse[2],tree_rmse[3])\n","          print('f_oneway:',test_stat,anova_p)\n","\n","        anova_data['trees'].iloc[num] = np.round(anova_p,3)    # 紀錄該參數的anova result\n","        best_parameter['trees'].iloc[num] = best_tree   # 紀錄該參數的最佳參數\n","        tree_result = [np.mean(tree_rmse[0]),np.mean(tree_rmse[1]),np.mean(tree_rmse[2]),np.mean(tree_rmse[3])]   # 紀錄每個參數之平均rmse\n","\n","########################################################################################################################################\n","\n","    elif param_name == 'sampling_sizes':\n","        sample_rmse = []\n","        best_rmse,best_para = 100,1000\n","        min_normality_p = 100\n","        for para in parameter:\n","            parm_set_2 = TWRF_kfold(tree=300,depth=20,max_sample=para,feature=12,seed = 47)\n","            print(para,normal_test(parm_set_2),parm_set_2)\n","\n","            # 找出最好的參數\n","            if np.mean(parm_set_2) < best_rmse:\n","              best_para = para\n","              best_rmse = np.mean(parm_set_2)\n","            sample_rmse.append(parm_set_2)\n","\n","            # 紀錄每個參數的常態檢定p-value\n","            bootstrap_size_data[str(para)].iloc[num] = normal_test(parm_set_2)\n","\n","            if normal_test(parm_set_2) < min_normality_p:\n","              min_normality_p = normal_test(parm_set_2)\n","\n","        if min_normality_p < 0.05 :\n","          test_stat,anova_p = kruskal(sample_rmse[0],sample_rmse[1],sample_rmse[2])\n","          print('kruskal_oneway:',test_stat,anova_p)\n","        else:\n","          test_stat,anova_p = f_oneway(sample_rmse[0],sample_rmse[1],sample_rmse[2])\n","          print('f_oneway:',test_stat,anova_p)\n","\n","        anova_data['bootstrap size'].iloc[num] = np.round(anova_p,3)\n","\n","        best_parameter['bootstrap size'].iloc[num] = best_para   # 紀錄該參數的最佳參數\n","        bootstrap_result = [np.mean(sample_rmse[0]),np.mean(sample_rmse[1]),np.mean(sample_rmse[2])]  # 紀錄每個參數之平均rmse\n","\n","########################################################################################################################################\n","\n","    elif param_name == 'depths':\n","        depth_rmse = []\n","        best_rmse,best_para = 100,1000\n","        for para in parameter:\n","            parm_set_2 = TWRF_kfold(tree=300,depth=para,max_sample=0.7,feature='auto',seed = 42)\n","            print(para,normal_test(parm_set_2),parm_set_2)\n","\n","            # 找出最好的參數\n","            if np.mean(parm_set_2) < best_rmse:\n","              best_para = para\n","              best_rmse = np.mean(parm_set_2)\n","            depth_rmse.append(parm_set_2)\n","\n","            # 紀錄每個參數的常態檢定p-value\n","            depth_data[str(para)].iloc[num] = normal_test(parm_set_2)\n","\n","        print(f_oneway(depth_rmse[0],depth_rmse[1],depth_rmse[2],depth_rmse[3],depth_rmse[4]))\n","\n","        # 紀錄該參數的anova result\n","        test_stat,anova_p = f_oneway(depth_rmse[0],depth_rmse[1],depth_rmse[2],depth_rmse[3],depth_rmse[4])\n","        anova_data['max depth'].iloc[num] = np.round(anova_p,3)\n","\n","        # 紀錄該參數的最佳參數\n","        best_parameter['max depth'].iloc[num] = best_para\n","\n","        # 紀錄每個參數之平均rmse\n","        depth_result = [np.mean(depth_rmse[0]),np.mean(depth_rmse[1]),np.mean(depth_rmse[2]),np.mean(depth_rmse[3]),np.mean(depth_rmse[4])]\n","\n","########################################################################################################################################\n","\n","    elif param_name == 'features':\n","        feature_rmse= []\n","        best_rmse,best_para = 100,1000\n","        for para in parameter:\n","            parm_set_2 = TWRF_kfold(tree=300,depth=30,max_sample=0.7,feature=para,seed = 42)\n","            print(para,normal_test(parm_set_2),parm_set_2)\n","\n","            # 找出最好的參數\n","            if np.mean(parm_set_2) < best_rmse:\n","              best_para = para\n","              best_rmse = np.mean(parm_set_2)\n","            feature_rmse.append(parm_set_2)\n","\n","            # 紀錄每個參數的常態檢定p-value\n","            max_feature_data[str(para)].iloc[num] = normal_test(parm_set_2)\n","\n","        print(f_oneway(feature_rmse[0],feature_rmse[1],feature_rmse[2],feature_rmse[3],feature_rmse[4]))\n","\n","        # 紀錄該參數的anova result\n","        test_stat,anova_p = f_oneway(feature_rmse[0],feature_rmse[1],feature_rmse[2],feature_rmse[3],feature_rmse[4])\n","        anova_data['max feature'].iloc[num] = np.round(anova_p,3)\n","\n","        # 紀錄該參數的最佳參數\n","        best_parameter['max feature'].iloc[num] = best_para\n","\n","        # 紀錄每個參數之平均rmse\n","        feature_result = [np.mean(feature_rmse[0]),np.mean(feature_rmse[1]),np.mean(feature_rmse[2]),np.mean(feature_rmse[3]),np.mean(feature_rmse[4])]\n","\n","print(tree_result)\n","print(bootstrap_result)\n","print(depth_result)\n","print(feature_result)"]},{"cell_type":"markdown","metadata":{"id":"CJ-ZtcVllQ1F"},"source":["# SRF"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"i0L4QW9blQ1G"},"outputs":[],"source":["def SRF_kfold(tree,depth,max_sample,feature,seed):\n","    RMSE_set_srf = []\n","    kf = KFold(n_splits=10, shuffle=True, random_state=seed)\n","    for train_index, valid_index in kf.split(scaled_x, data_y): # split()  method generate indices to split data intSo training and test set.\n","        #print('fold',cnt)\n","        train_x = scaled_x.iloc[train_index,:]\n","        train_y = data.iloc[train_index,:]['stab']\n","        valid_x = scaled_x.iloc[valid_index,:]\n","        valid_y = data.iloc[valid_index,:]['stab']\n","\n","        SRF = Strength_Random_Forest(train_x , train_y, valid_x,max_depth=depth,sample_size=max_sample,n_trees=tree, n_features = feature )\n","        RMSE_set_srf.append( np.sqrt( mean_squared_error(valid_y,SRF)))\n","\n","    mean_rmse = np.mean(RMSE_set_srf)\n","    return RMSE_set_srf\n","\n","def Strength_Random_Forest(training_x , training_y, test_x, max_depth, sample_size, n_trees, n_features):\n","    strength_set = []\n","    test_predicted_ys = []\n","    for i in range(n_trees):\n","        train_x, valid_x ,train_y ,valid_y = train_test_split(training_x, training_y , train_size = sample_size) #隨機選擇n%的樣本\n","        DT = DecisionTreeRegressor(max_depth = max_depth , max_features=n_features )\n","        DT.fit(train_x,train_y)\n","        dt_pred = DT.predict(valid_x)\n","\n","        confidence = []\n","        margin = np.abs(dt_pred - valid_y)\n","        for j in range(len(margin)):\n","            confidence.append(1/ math.exp(margin.values[j]))\n","        strength = np.sum(confidence)/len(confidence)\n","        strength_set.append(strength)\n","\n","        test_pred = DT.predict(test_x)\n","        test_predicted_ys.append(test_pred)\n","\n","        final_result = []\n","    for i in range(len(test_predicted_ys[0])): #y的數量\n","        predict_result = [] #存放每棵樹的預測值 #100棵樹有100個預測值\n","        for j in range(len(test_predicted_ys)):\n","            predict_result.append( (test_predicted_ys[j][i]) )\n","        strength_predict_result = np.array(predict_result) * np.array(strength_set) #每顆樹的預測值*每個樹的strength\n","        final_result.append( np.mean(strength_predict_result)  ) #存放最後的2000個預測值\n","\n","    return final_result"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Y8_PQ6PXlQ1G","outputId":"0cc610e4-2e97-4217-c8e4-5d41ea4a27d9"},"outputs":[{"name":"stdout","output_type":"stream","text":["================ trees ================\n","100 0.683 [0.011823254219488065, 0.011919221891035264, 0.011897707938725653, 0.011774322645500076, 0.01201010070962436, 0.011756940562924643, 0.011971514620222237, 0.01162372417646224, 0.011702198068090388, 0.011962342513005193]\n","300 0.649 [0.011864201290861296, 0.011786899372150574, 0.011834700061749867, 0.011664298945124966, 0.011978425019831259, 0.011695940248138574, 0.011886829882686106, 0.01149446603627862, 0.011579632779732865, 0.011890431629316882]\n","500 0.786 [0.011840667191075335, 0.01185073756447224, 0.011783493937587392, 0.011612748275932054, 0.011949854173442156, 0.011725523230848784, 0.011820465013315332, 0.011629981902920023, 0.01153834479287051, 0.011984653458980313]\n","700 0.398 [0.011817658367488744, 0.011795867352860472, 0.011794523322880834, 0.011635345310345453, 0.011970613546518714, 0.011726235317189224, 0.011933946552715262, 0.011515616316295468, 0.011530527367098762, 0.011985119649999073]\n","f_oneway: 0.6013153201196215 0.6183633995001558\n","================ sampling_sizes ================\n","0.5 0.035 [0.011873637487697219, 0.011872759944462683, 0.011786965096679952, 0.011628237782060867, 0.011909770601715119, 0.011611704237622648, 0.011920431716660714, 0.011657612296538942, 0.011655117051627358, 0.01192753484809981]\n","0.7 0.531 [0.011786570467865161, 0.011774192614131556, 0.011785807112426024, 0.011620799983506234, 0.012090853119845486, 0.011777633248128385, 0.011880211942703351, 0.011547424589786063, 0.011573403047120179, 0.011909248771065426]\n","0.9 0.123 [0.01232133665685506, 0.012297308442163118, 0.012234056685073467, 0.012441753754726127, 0.012886431237608744, 0.012236999754909269, 0.012216178690382629, 0.012602266541014916, 0.012127241339396551, 0.012779681500507019]\n","kruskal_oneway: 19.56387096774195 5.646240814194361e-05\n","================ depths ================\n","10 0.16 [0.01335170921697256, 0.013146091342756407, 0.013224805939383045, 0.012564692857607357, 0.01271276984893778, 0.012991388677633546, 0.012603029233383247, 0.012857859461334218, 0.012603994128457795, 0.013971285925172413]\n","20 0.249 [0.012060956121652087, 0.011923779839800627, 0.01193768873378564, 0.011447400098715383, 0.011511346600174624, 0.011742483590049878, 0.01129249746603442, 0.011587870379495145, 0.011566689269101958, 0.012667517595251962]\n","30 0.244 [0.012111152198042032, 0.011873409622988166, 0.011961732900616811, 0.0114834334870621, 0.011575560143550155, 0.011785855045096936, 0.011296743675192854, 0.011584999630787931, 0.011465761327093973, 0.012705661951958673]\n","40 0.224 [0.012146051094600882, 0.011963882682524365, 0.011992410009902749, 0.01145555604200884, 0.011494613250090298, 0.011742414077395014, 0.011332091591587736, 0.011631683164071902, 0.011520488253249025, 0.01271554261329451]\n","50 0.419 [0.012070416772701981, 0.011836189405751768, 0.011967747053684478, 0.011402720750855801, 0.011464810779825807, 0.011774414461279803, 0.011279242000346693, 0.011670035718505336, 0.011550669497201443, 0.01264835719001745]\n","F_onewayResult(statistic=17.53598798164513, pvalue=9.702198198146186e-09)\n","================ features ================\n","4 0.762 [0.012392409570401101, 0.012250285785196935, 0.01210392020063848, 0.011563906892089996, 0.01160658136251475, 0.011946547153752292, 0.011422712812924793, 0.011941079773665342, 0.011713882225975527, 0.012805764729300338]\n","6 0.557 [0.012111782136600009, 0.011843363640678796, 0.011789266133883163, 0.011284179965723146, 0.011286974200751435, 0.011679462508195446, 0.011098341508006062, 0.011540915087770174, 0.011308702531681153, 0.012485186768099905]\n","8 0.544 [0.012068341412449026, 0.011802489456303656, 0.011810431250800143, 0.011421682759860002, 0.011273383528814263, 0.011706675409844533, 0.011056992832013907, 0.011442708131660314, 0.011310031622974785, 0.012590325327871757]\n","10 0.496 [0.012072420127506062, 0.011870903029263736, 0.011948664045185962, 0.011479901912381834, 0.011372116014218326, 0.011699527994374686, 0.011154998510933782, 0.01143384082547472, 0.01143590820449458, 0.012564418984806485]\n","12 0.202 [0.01211910669574947, 0.011912873104162534, 0.011986071564213144, 0.011436511122355575, 0.011496569456338158, 0.011800624127619152, 0.011355311985795001, 0.011589844581124963, 0.011522211058130009, 0.012674325151266918]\n","F_onewayResult(statistic=1.0503339150461921, pvalue=0.3920813658147142)\n","[0.011844132734507812, 0.011767582526587102, 0.011773646954144413, 0.011770545310339203]\n","[0.011784377106316532, 0.011774614489657786, 0.012414325460263689]\n","[0.013002762663163839, 0.011773822969406173, 0.011784430998238962, 0.011799473277872532, 0.011766460363017057]\n","[0.011974709050645956, 0.011642817448138928, 0.01164830617325924, 0.011703269964864017, 0.011789344884675491]\n"]}],"source":["num = 3\n","\n","for param_name, parameter in rf_paremater_set.items():\n","    print('================',param_name,'================')\n","    if param_name == 'trees':\n","        tree_rmse  = []\n","        best_rmse,best_para = 100,1000\n","        min_normality_p = 100\n","        for para in parameter:\n","            parm_set_2 = TWRF_kfold(tree=para,depth=None,max_sample=0.7,feature=12,seed = 47 ) #23 31 47 52\n","            print(para,normal_test(parm_set_2),parm_set_2)\n","\n","            # 找出最好的樹參數\n","            if np.mean(parm_set_2) < best_rmse:\n","              best_tree = para\n","              best_rmse = np.mean(parm_set_2)\n","            tree_rmse.append(parm_set_2)\n","\n","            # 紀錄每個參數的常態檢定p-value\n","            tree_data[str(para)].iloc[num] = normal_test(parm_set_2)\n","\n","            if normal_test(parm_set_2) < min_normality_p:\n","              min_normality_p = normal_test(parm_set_2)\n","\n","        if min_normality_p < 0.05 :\n","          test_stat,anova_p = kruskal(tree_rmse[0],tree_rmse[1],tree_rmse[2],tree_rmse[3])\n","          print('kruskal_oneway:',test_stat,anova_p)\n","        else:\n","          test_stat,anova_p = f_oneway(tree_rmse[0],tree_rmse[1],tree_rmse[2],tree_rmse[3])\n","          print('f_oneway:',test_stat,anova_p)\n","\n","        anova_data['trees'].iloc[num] = np.round(anova_p,3)    # 紀錄該參數的anova result\n","        best_parameter['trees'].iloc[num] = best_tree   # 紀錄該參數的最佳參數\n","        tree_result = [np.mean(tree_rmse[0]),np.mean(tree_rmse[1]),np.mean(tree_rmse[2]),np.mean(tree_rmse[3])]   # 紀錄每個參數之平均rmse\n","\n","########################################################################################################################################\n","\n","    elif param_name == 'sampling_sizes':\n","        sample_rmse = []\n","        best_rmse,best_para = 100,1000\n","        min_normality_p = 100\n","        for para in parameter:\n","            parm_set_2 = TWRF_kfold(tree=300,depth=20,max_sample=para,feature=12,seed = 47)\n","            print(para,normal_test(parm_set_2),parm_set_2)\n","\n","            # 找出最好的參數\n","            if np.mean(parm_set_2) < best_rmse:\n","              best_para = para\n","              best_rmse = np.mean(parm_set_2)\n","            sample_rmse.append(parm_set_2)\n","\n","            # 紀錄每個參數的常態檢定p-value\n","            bootstrap_size_data[str(para)].iloc[num] = normal_test(parm_set_2)\n","\n","            if normal_test(parm_set_2) < min_normality_p:\n","              min_normality_p = normal_test(parm_set_2)\n","\n","        if min_normality_p < 0.05 :\n","          test_stat,anova_p = kruskal(sample_rmse[0],sample_rmse[1],sample_rmse[2])\n","          print('kruskal_oneway:',test_stat,anova_p)\n","        else:\n","          test_stat,anova_p = f_oneway(sample_rmse[0],sample_rmse[1],sample_rmse[2])\n","          print('f_oneway:',test_stat,anova_p)\n","\n","        anova_data['bootstrap size'].iloc[num] = np.round(anova_p,3)\n","\n","        best_parameter['bootstrap size'].iloc[num] = best_para   # 紀錄該參數的最佳參數\n","        bootstrap_result = [np.mean(sample_rmse[0]),np.mean(sample_rmse[1]),np.mean(sample_rmse[2])]  # 紀錄每個參數之平均rmse\n","\n","########################################################################################################################################\n","\n","    elif param_name == 'depths':\n","        depth_rmse = []\n","        best_rmse,best_para = 100,1000\n","        for para in parameter:\n","            parm_set_2 = TWRF_kfold(tree=300,depth=para,max_sample=0.7,feature='auto',seed = 42)\n","            print(para,normal_test(parm_set_2),parm_set_2)\n","\n","            # 找出最好的參數\n","            if np.mean(parm_set_2) < best_rmse:\n","              best_para = para\n","              best_rmse = np.mean(parm_set_2)\n","            depth_rmse.append(parm_set_2)\n","\n","            # 紀錄每個參數的常態檢定p-value\n","            depth_data[str(para)].iloc[num] = normal_test(parm_set_2)\n","\n","        print(f_oneway(depth_rmse[0],depth_rmse[1],depth_rmse[2],depth_rmse[3],depth_rmse[4]))\n","\n","        # 紀錄該參數的anova result\n","        test_stat,anova_p = f_oneway(depth_rmse[0],depth_rmse[1],depth_rmse[2],depth_rmse[3],depth_rmse[4])\n","        anova_data['max depth'].iloc[num] = np.round(anova_p,3)\n","\n","        # 紀錄該參數的最佳參數\n","        best_parameter['max depth'].iloc[num] = best_para\n","\n","        # 紀錄每個參數之平均rmse\n","        depth_result = [np.mean(depth_rmse[0]),np.mean(depth_rmse[1]),np.mean(depth_rmse[2]),np.mean(depth_rmse[3]),np.mean(depth_rmse[4])]\n","\n","########################################################################################################################################\n","\n","    elif param_name == 'features':\n","        feature_rmse= []\n","        best_rmse,best_para = 100,1000\n","        for para in parameter:\n","            parm_set_2 = TWRF_kfold(tree=300,depth=30,max_sample=0.7,feature=para,seed = 42)\n","            print(para,normal_test(parm_set_2),parm_set_2)\n","\n","            # 找出最好的參數\n","            if np.mean(parm_set_2) < best_rmse:\n","              best_para = para\n","              best_rmse = np.mean(parm_set_2)\n","            feature_rmse.append(parm_set_2)\n","\n","            # 紀錄每個參數的常態檢定p-value\n","            max_feature_data[str(para)].iloc[num] = normal_test(parm_set_2)\n","\n","        print(f_oneway(feature_rmse[0],feature_rmse[1],feature_rmse[2],feature_rmse[3],feature_rmse[4]))\n","\n","        # 紀錄該參數的anova result\n","        test_stat,anova_p = f_oneway(feature_rmse[0],feature_rmse[1],feature_rmse[2],feature_rmse[3],feature_rmse[4])\n","        anova_data['max feature'].iloc[num] = np.round(anova_p,3)\n","\n","        # 紀錄該參數的最佳參數\n","        best_parameter['max feature'].iloc[num] = best_para\n","\n","        # 紀錄每個參數之平均rmse\n","        feature_result = [np.mean(feature_rmse[0]),np.mean(feature_rmse[1]),np.mean(feature_rmse[2]),np.mean(feature_rmse[3]),np.mean(feature_rmse[4])]\n","\n","print(tree_result)\n","print(bootstrap_result)\n","print(depth_result)\n","print(feature_result)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BPiZAVRRlQ1H"},"outputs":[],"source":["depth_data.to_csv(\"depth.csv\",index=False)\n","tree_data.to_csv(\"tree.csv\",index=False)\n","max_feature_data.to_csv(\"max feature.csv\",index=False)\n","bootstrap_size_data.to_csv(\"bootstrap size.csv\",index=False)\n","best_parameter.to_csv(\"bast parameter.csv\",index=False)\n","anova_data.to_csv(\"anova result.csv\",index=False)"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.16"}},"nbformat":4,"nbformat_minor":0}